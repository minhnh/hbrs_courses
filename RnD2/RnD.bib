% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Bohg2014,
  Title                    = {Data-Driven Grasp Synthesis: A Survey},
  Author                   = {J. Bohg and A. Morales and T. Asfour and D. Kragic},
  Journal                  = {IEEE Transactions on Robotics},
  Year                     = {2014},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {289-309},
  Volume                   = {30},

  Abstract                 = {We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps. We divide the approaches into three groups based on whether they synthesize grasps for known, familiar, or unknown objects. This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique. In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation. In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects. Finally, for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps. Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping. We also draw a parallel to the classical approaches that rely on analytic formulations.},
  Doi                      = {10.1109/TRO.2013.2289018},
  ISSN                     = {1552-3098},
  Keywords                 = {feature extraction;grippers;image matching;object recognition;pose estimation;sampling methods;candidate grasp ranking;candidate grasp sampling;common object representations;data-driven grasp synthesis technique;feature extraction;object recognition;perceptual processes;pose estimation;robot grasping;similarity matching;Databases;Feature extraction;Grasping;Measurement;Robot sensing systems;Grasp planning;grasp synthesis;object grasping and manipulation;object recognition and classification;visual perception;visual representations}
}

@Article{Goldfeder2011,
  Title                    = {Data-driven grasping},
  Author                   = {Goldfeder, Corey
and Allen, Peter K.},
  Journal                  = {Autonomous Robots},
  Year                     = {2011},

  Month                    = {Jul},
  Number                   = {1},
  Pages                    = {1--20},
  Volume                   = {31},

  Abstract                 = {This paper propose a novel framework for a data driven grasp planner that indexes partial sensor data into a database of 3D models with known grasps and transfers grasps from those models to novel objects. We show how to construct such a database and also demonstrate multiple methods for matching into it, aligning the matched models with the known sensor data of the object to be grasped, and selecting an appropriate grasp to use. Our approach is experimentally validated in both simulated trials and trials with robots.},
  Day                      = {01},
  Doi                      = {10.1007/s10514-011-9228-1},
  ISSN                     = {1573-7527},
  Url                      = {https://doi.org/10.1007/s10514-011-9228-1}
}

@Article{mahler2017,
  Title                    = {Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point
 Clouds and Analytic Grasp Metrics},
  Author                   = {Jeffrey Mahler and
 Jacky Liang and
 Sherdil Niyaz and
 Michael Laskey and
 Richard Doan and
 Xinyu Liu and
 Juan Aparicio Ojea and
 Ken Goldberg},
  Journal                  = {CoRR},
  Year                     = {2017},
  Volume                   = {abs/1703.09312},

  Abstract                 = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net.},
  Archiveprefix            = {arXiv},
  Eprint                   = {1703.09312},
  Timestamp                = {Wed, 07 Jun 2017 14:41:15 +0200},
  Url                      = {http://arxiv.org/abs/1703.09312}
}


%!TEX root = ../report.tex

\chapter{State of the Art}

Empirical grasp synthesis techniques based on labeled data vary mainly in what type of perceptual data is taken into
consideration, how object-grasp representation is formulated based on such perceptual data, and how grasps are
evaluated. This section will review these aspects are handled in recent methods, as well as approaches to augment and
synthesize data relevant to grasping.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extracting features from perceptual data for grasping}
As RGB-D cameras become more accessible and affordable, they are often chosen in robotic systems as the primary
perception sensor. While RGB-D data has been shown to provide richer features compared to pure 2D images for various
perception tasks \cite{lenz2015,Eitel2015,Gupta2014RGBDFeatures,jiang2011}, it is often unclear how to handle the
multi-modality of color and depth information.

Bo et al. \cite{Bo2013} build sparse coding dictionaries for RGB-D data using the K-SVD algorithm and proposed to use
a hierarchical matching pursuit (HMP)  algorithm to compute a feature hierarchy for new RGB-D images. Each entry in the
dictionaries contain 8 channels calculated from RGB-D data: grayscale intensity, RGB, depth and surface normals in three
axes.

Lenz et al. \cite{lenz2015} use deep auto-encoders to build a representation for each feature channel, reducing its data
dimensions. To handle the multi-modality of RGB and depth data, the authors also introduced a structured regularization
technique. Each modality (i.e. RGB and depth/surface normal) has a regularization term which will be added to each
hidden unit. In case of a $p-norm$, with $K$ hidden units, $R$ modalities, and N visible units, the regularization term
would be
\[f(W) = \sum\limits^K_{j=1} \sum\limits^R_{r=1} \left( \sum\limits^N_{i=1} S_{r,i} \lvert W^p_{i,j} \rvert \right)
^{1/p}, \]
where $S_{r,i}$ is $1$ if feature $i$ belongs to modality $r$ and $0$ otherwise.

\picHereWidth{Eitel_et_al-2015-depth_color_encodings}
{Different encodings of depth information \cite{Eitel2015}. From left: RGB, grayscale depth, surface
    normals \cite{Bo2013}, HHA \cite{Gupta2014RGBDFeatures}, and the color mapping of depth values proposed
    by Eitel et al. \cite{Eitel2015}.}
{fig:depth-encodings}{\linewidth}

Building upon the success of Convolutional Neural Networks (CNN) in capturing 2D features \cite{Gu2018}, several
multi-modal approaches convert depth data into three-channel images during the preprocessing step
\cite{Eitel2015,Gupta2014RGBDFeatures}. Gupta et al. \cite{Gupta2014RGBDFeatures} propose the HHA representation, which
encodes the depth each pixel of the depth image with horizontal disparity, height above ground, and the angle between
the local surface normal and direction of gravity. Eitel et al. \cite{Eitel2015} propose to use convert the depth value
directly into RGB values using a jet color mapping. Examples of the different depth encodings can be seen in figure
\ref{fig:depth-encodings}. Two CNN models of the same architecture are then trained independently using depth and RGB
images to classify objects. Finally, features learned from the two models (the networks up until before the
classification layer) are concatenated as inputs to a fully connected layer which learns to combine them to recognize
objects. The paper showed that while surface normals proved to give better classification performance using depth
information alone, they require additional calculation on the input data and does not improve on results when combining
both RGB and depth images compared to the proposed color map encoding.

More recently, Porzi et al. \cite{Porzi2017} introduce a novel convolutional block called \emph{DaConv} to learn scale
awareness using depth information. Each \emph{DaConv} block consists of two networks, namely \emph{PredictionNet} and
\emph{DepthNet}. In \emph{PredictionNet} different scales of the input features are simulated using dilated convolutions
\cite{YuKoltun2016} with different dilation factors $ \{ \ell_1,...,\ell_d \} $. An $ \ell $-dilated convolution can be
obtained by adding $ \ell - 1 $ zeros between adjacent elements of the convolutional kernel. \emph{DepthNet} use only
the depth information to produce a vector of probabilities $ \{ a_1,...,a_d \} $ representing how likely a dilation
factor is chosen for a specific spatial location, captured by each convolutional kernel. The outputs of the dilated
convolutions are then linearly combined using this probability vector to produce the final output of the \emph{DaConv}
block. The authors evaluate their networks on three robot perception tasks and achieve state-of-the-art performance. The
three perception tasks examined are object pose estimation, object affordance detection, and contour detection.

Above methods deal with data acquired from a single RGB-D sensor, which does not provide complete 3D information, as
occluded regions are not included in point clouds. Techniques generating representations for object models, however,
work with true 3D information and can give insights into extracting features from a single view RGB-D camera. Recent
techniques for generating representations of 3D object models generally use either volumetric CNN's (where the kernels
have three dimensions instead of two like regular CNN's) or applying well known CNN models to 2D views rendered from the
3D model. Qi et al. \cite{Qi2016} develop two novel volumetric architectures, which significantly improve previous
volumetric CNN results. The first approach introduces auxiliary learning tasks in the final layers, training parts of
the network only on sub-volumes of the full objects. The second approach introduces anisotropic probing kernels to
project the 3D model onto 2D views, then applies a 2D CNN model on the projections. The authors also extend and improve
upon a highly successful multi-view CNN technique proposed by Su et al. \cite{Su2015} by introducing multi-
resolution filters to capture information at multiple scales. These techniques may allow sampling occluded points in a
single-view RGB-D image, which would provide more information for planning grasps.

Efforts have also been spent to reconstruct objects from the incomplete view of a single RGB-D camera. Bohg et al.
\cite{Bohg2011MindTheGap} search for a symmetric plane perpendicular to workspace surface (i.e. table) and mirror
the incomplete point cloud across this plane to fill in the missing information. Varley et al. \cite{Varley2017}
train a CNN to take in an occupancy grid computed from a 2.5D point cloud and output an occupancy grid of the same shape
which predicts if the obscured points are occupied. This output voxel grid is either directly run through a marching
cubes algorithm to quickly create an object mesh and add it to the manipulation planning scene, or post-processing
is applied to create a higher resolution mesh for grasp planning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Object-Grasp representation}
Bohg et al. \cite{Bohg2014} parameterize grasps by (1) the \emph{grasping point} of the object where the gripper
should be aligned, (2) an \emph{approach vector} from which the gripper shall approach the \emph{grasping point},
(3) the \emph{wrist orientation} of the robotic hand, and (4) an \emph{initial gripper configuration}.
The article also categorized object-grasp representation approaches into ones which extract local (i.e. curvature,
contact area with the hand) or global features (center of mass, bounding box).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Techniques based on global features}
As illustrated in figure \ref{fig:dexnet-data-gen}, Mahler et al. \cite{mahler2017} represent grasps in 2D images by
aligning the image center to the gripper central point and the image's middle row to the grasp axis. The grasp is
assumed to have an approach vector perpendicular to the table and hence is characterized by the gripper center and the
angle of the gripper axis with respect to the table.
\picHereWidth{mahler_et_al-2017-dexnet_2-dataset_generation}
{Dex-Net 2.0 data generation pipeline \cite{mahler2017}.}
{fig:dexnet-data-gen}{\textwidth}

Several methods use eigengrasps to represent the object-grasp relation in free space \cite{Goldfeder2011,Ciocarlie2009}.
Ciocarlie and Allen \cite{Ciocarlie2009} introduced eigengrasps and defined them as principal components of the dataset
of human hand configurations. Using this low-dimensional representation, the robot hand receive adjustments from a human
operator during grasp execution. The proposed Eigengrasp planner uses a simulated annealing algorithm to maximize a
quality metric $Q$ calculated from the robot hand's posture and pose, where posture is represented using an eigengrasp.
Specifically,
\[ Q = \Sigma_i (1 - \delta_i) \]
for all desired contacts $i$, where
\[ \delta_i = \cfrac{|\mathbf{o_i}|}{\alpha}
+ \left(1 - \cfrac{\mathbf{\hat{n}_i} \cdot \mathbf{o_i}}{|\mathbf{o_i}|} \right), \]
in which the local surface normal $\mathbf{\hat{n}_i}$ and the distance between the desired contact location and the
object $\mathbf{o_i}$ is related to the eigengrasp and hand position. $\alpha$ is a scaling factor make the first term
comparable to the second in the latter equation. Goldfeder and Allen \cite{Goldfeder2011} used the Eigengrasp planner
to generate the Columbia Grasp Database. Grasp planning is then done by matching the query object to one in the
database, then executing the best indexed grasp for the matched object.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Techniques based on local features} \label{subsub:object_grasp_local}

\picHereWidth{detry_et_al-2012-fig3-part_candidates}
{Generating part candidates with box-shaped region of interests \cite{Detry2012}.}
{fig:part_candidates}{0.7\textwidth}

Detry et al. \cite{Detry2012} attempt to apply grasp experience demonstrated on known objects to novel ones by finding
similar graspable regions. As illustrated in figure \ref{fig:part_candidates}, part candidates are point clouds
extracted from applying a set of predefined regions of interest (ROI) boxes on a known grasp from the database. Then the
authors defined a distance metric for clustering similar candidate parts, and a dictionary of candidate parts is created
using only the central points of these clusters. Part candidates for new object are then matched against this dictionary
to find possible grasps for the familiar object part.

Several methods represent grasp candidates as rectangles positioned in RGB and/or depth images
\cite{lenz2015,jiang2011}, corresponding with a bipedal gripper. The RGB-D image region within these rectangles is then
used to score the respective grasp using machine learning methods. These approaches are naturally limited in their
ability to represent the robot hand's approach vector since rectangles in images can only correspond to grasps roughly
parallel with the RGB-D camera's optical axis \cite{Gualtieri2016}.

\picHereWidth{kappler_et_al-2015-fig8-local_shape_diff_viewpoints}
{Local shape representations with different camera viewpoints of the same grasp. Cyan lines represent the
    approach vectors while pink lines represent the camera viewpoint \cite{Kappler2015}.}
{fig:local_shape_viewpoints}{0.8\textwidth}

Kappler et al. \cite{Kappler2015} extract local shape representations (or templates) of objects from point clouds
generated from object meshes at various camera viewpoints. The templates are grids with predefined resolution
aligned with the plane tangent to the object surface at the intersection point between the grasp approach vector and the
object. Each point cloud will be projected onto the grid, where the grid cells with corresponding projected points
``contain the distance to that point as a value,'' and the other grid cells are considered either occlusion
(with height depends on the camera angle) or free space (with a fixed height). Figure \ref{fig:local_shape_viewpoints}
illustrates how the local shape representations capture the different camera viewpoints.

Gualtieri et al \cite{Gualtieri2016} propose to represent a grasp candidate by the cuboid swept out by a two-fingered
gripper as it closes on the object, extracting the observed points from RGB-D clouds and sampling points unobserved by
all sensors within the region. In order to reduce dimensionality, the points in this cuboid region are then voxelized
into $60 \times 60 \times 60$ grid, then projected onto the three planes corresponding with the cuboid's principal axes.
For each projection, three images are generated: the average height maps of the occupied points ($60 \times 60$) and
unobserved region ($60 \times 60$), as well as the average surface normals ($60 \times 60 \times 3$), whose three
dimensions are interpreted as three channels in the image. These result in a total of 15 channels representing each
candidate grasp. In their experiments, this approach performs better than the above approach proposed by Kappler et al
\cite{Kappler2015}, but does not improve significantly over their previous approach, which used only three channels from
the surface normals for each projection. This approach did not consider RGB data, arguing that the improvement would be
insignificant considering the results from \cite{lenz2015}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grasp evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background and terminology}
\picHereWidth{roa_suarez-2015-grasp_relations}
{Relationships between velocities and forces, torques for multi-fingered grasping \cite{Roa2015}.}
{fig:grasp_relations}{0.7\textwidth}

Many classical grasp evaluation approaches follow Murray et al \cite{Murray1994}'s definition of grasp planning as the
problem of finding a set of contact locations between a rigid-body object and the end-effector's fingers, mounted on a
rigid-link robot. The model of contact between the object's surface and robot fingertips affects the mechanical analysis
of grasps. Contact types are generally grouped into three categories:
\begin{itemize}
    \item \emph{Frictionless point contact}: applied forces are assumed to always be normal to the object's surface.
    \item \emph{Point contact with friction}: applied forces are assumed to have a normal and a tangential component,
    the latter representing friction between the fingers and the object surface. This contact is commonly modeled by the
    Coulomb's friction cone.
    \item \emph{Soft contact}: in addition to the forces in the friction cone described in the previous model, torques
    around the object's surface normal are also taken into consideration.
\end{itemize}

In their survey of grasp metrics, Roa and Su{\'a}rez \cite{Roa2015} adopt the following notations for concepts defined
in \cite{Murray1994}. Force $ \boldsymbol{F}_i $ applied on the object at contact point $ \boldsymbol{p}_i $ generate a
torque $ \boldsymbol{\tau}_i $, and are modeled by a wrench vector
$ \boldsymbol{\omega}_i = (\boldsymbol{F}_i, \boldsymbol{\tau}_i/\rho)^T $, in which $ \rho $ is a constant that defines
the metrics of the wrench space. Contact type dictates the number of independent components $ r $ of
$ \boldsymbol{\omega}_i $: $ r=1 $ for frictionless contacts, $ r = 2 $ or $ 3 $ for contacts with friction for 2D and
3D objects, and $ r = 4 $ for soft contacts.

The object's movement is modeled by a twist $ \boldsymbol{\dot{x}} = (\boldsymbol{v}, \boldsymbol{w})^T $, where
$ \boldsymbol{v} $ is the translational velocity of the object's center of mass (CM), and $ \boldsymbol{w} $ is the
object's rotational velocity around its CM.

The robot hand is assumed to have $ n $ fingers, each with $ m $ joints. The force $ \boldsymbol{f}_i $ applied at
fingertip $ i $, $ i = 1,...,n $, is caused by the torques in each joint $ \boldsymbol{T}_{ij} $, $ j = 1,...,m$.
$ \boldsymbol{\theta}_{ij} $ denotes the velocities in these finger joints. Vectors $ \boldsymbol{f} $ and
$ \boldsymbol{\nu} $ denote the forces and velocities at the fingertips after grouping all the $ r $ independent
components together.

The relations between variables at the finger joints and at contact points are captured by the hand Jacobian
$ J_h $:
\begin{equation} \label{eq:hand_jacobian1}
\boldsymbol{\nu} = J_h \boldsymbol{\dot{\theta}}
\end{equation}
\begin{equation} \label{eq:hand_jacobian2}
\boldsymbol{T} = J_h^T \boldsymbol{f}
\end{equation}
, whereas the relationship between the object and the contact points are captured by the grasp matrix $ G $:
\begin{equation} \label{eq:grasp_matrix1}
\boldsymbol{\nu} = G^T \boldsymbol{\dot{x}}
\end{equation}
\begin{equation} \label{eq:grasp_matrix2}
\boldsymbol{\omega} = G \boldsymbol{f}
\end{equation}
. These relationships can be seen visualized in figure \ref{fig:grasp_relations}. The fundamental grasping constraint
can then be derived from equations (\ref{eq:hand_jacobian1}) and (\ref{eq:grasp_matrix1}):
\begin{equation} \label{eq:grasp_constraint}
J_h \boldsymbol{\dot{\theta}} = G^T \boldsymbol{\dot{x}}
\end{equation}
. The hand-object Jacobian $ H $ represents the transformation between the high-dimensional hand joint space and the
low-dimensional object space:
\begin{equation} \label{eq:hand_object_jacobian}
\boldsymbol{\dot{x}} = H \boldsymbol{\dot{\theta}}
\end{equation}
, where $ H = (G^T)^+ J_h $, and $ (G^T)^+ $ denotes the pseudoinverse of $ G^T $.

Via approximating the friction cones at contact points $ \boldsymbol{p}_i $ as pyramids, the applied wrench can be
represented by a linear combination of primitive wrenches. The convex hull $ \mathcal{P} $ containing all primitive
wrenches is called the \emph{Grasp Wrench Space} (GWS).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analytical grasp metrics}

Roa and Su{\'a}rez \cite{Roa2015} group grasp quality measures into approaches focusing on contact point position, hand
configuration, and ones which combine both metric types.

Contact point grasp quality measures focus on the object's properties, friction constraints, and form/force closure
conditions. Here the space of all force-closure grasps is referred to as \emph{force-closure space} (FCS). Several of
these approaches analyze the algebraic properties of the grasp matrix $ G $, most commonly optimizing some function of
its singular values. Other approaches may measure grasp quality based on geometric relations between the robot hand and
the object, such as shape or area of grasp polygon for 2D objects, distance between the contact polygon's centroid and
the object's CM, how well the hand aligns with the object's principal axis, or how far away the finger positions are
from the boundaries of the FCS. Another factor which may affect grasp quality is the limits of finger forces. Methods
concerning this factor may consider the geometric properties of the GWS $ \mathcal{P} $, or formulate the metrics
directly from the applied forces and torques. Task-oriented measures may focus on analyzing wrenches within a
\emph{task wrench space} (TWS), also known as task polytope, which includes wrenches needed for a specific task to be
executed or disturbance wrenches expected during task execution.

Approaches focusing on hand configuration measures focus on studying the properties of the hand-object Jacobian $H$,
many of which apply a similar analysis of the algebraic properties of the grasp matrix $ G $ to $ H $. Some approaches
in this category also consider the physical limits of the end-effector fingers, ranking grasps based on how close each
joint is to its operating range.

Grasp measures can be combined serially or in parallel to capture different aspects of a grasp's quality. In the serial
case, one measure is first applied to find several good grasps, then another is used to choose the optimal candidate
from these grasps. A parallel approach combines the different measures in a single global index, i.e. simply calculating
a normalized sum of these measures or creating a ranking table for each quality criterion, then summing the rankings
for each grasp.

Most popular among analytical grasp evaluation methods is the $\epsilon$-metric. Ferrari and Canny
\cite{FerrariCanny1992} formulated the metric from the maximum external wrench that a grasp can resist in any
direction. In wrench space $ \mathcal{P} $ this value is represented by the distance from the space's origin to its
convex hull. The metric, therefore, is geometrically characterized by the radius of the largest sphere that can be
contained in $ \mathcal{P} $ and centered at its origin \cite{Roa2015}. Weisz and Allen \cite{WeiszAllen2012} extend
the $\epsilon$-metric with probabilistic methods to better deal with uncertainties in object pose. The method assumes
the object to be fixed on a table and limit their noise model to the 2D coordinates and an angle with respect to this
table, $ [x, y, \theta] $. Grasp simulations are run for all perturbations of the object pose within predefined error
ranges for these three parameters. The extended grasp metric is defined as the probability of that the evaluated
grasp achieves an $ \epsilon $ score over a certain value, for all perturbations. Evaluation using this metric shows
that grasps with the best $ \epsilon $ score is much more likely to have low $ \epsilon $ score when pose variations
are introduced, as compared to grasps which perform well under the proposed metric. However, the authors also admit
that the calculation of the metric is intractable in real time, taking hours in worst cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning to predict grasp quality}

\begin{table}[h!]
    \small
    \begin{tabularx}{\textwidth}{L{0.12\textwidth}L{0.26\textwidth}L{0.33\textwidth}L{0.26\textwidth}}
        \cmidrule[0.08em](){1-4}
        Approach                              & Object-grasp representation & Feature extraction \& learning method
        & Dataset generation
        \\ \cmidrule[0.08em](){1-4}
        Jiang et al. \cite{jiang2011}         & Rectangle in RGB-D image
        & Histograms of hand-crafted filters of RGB-D images
        \linebreak Model: SVM
        & Rectangles manually annotated on object images
        \\ \cmidrule[0.01em](){1-4}
        Lenz et al. \cite{lenz2015}           & Rectangle in RGB-D image
        & Auto-encoders used to initialize weights, structured regularization used
        to combine depth and RGB data
        \linebreak Model: MLP
        & Extended version of the dataset used in Jiang et al. \cite{jiang2011}
        \\ \cmidrule[0.01em](){1-4}
        Kappler et al. \cite{Kappler2015}     & Template grid at intersection of object surface and approach vector
        & RGB rendering of the template grid
        \linebreak Model: LeNet CNN
        & Grasps and quality values are generated for object meshes in simulation
        then verified by crowd- sourcing
        \\ \cmidrule[0.01em](){1-4}
        Gualtieri et al. \cite{Gualtieri2016} & Rectangle cuboid region swept out by the gripper fingers, containing
        observed depth data and points sampled from the occluded volume
        & Different filters of the cuboid region are projected onto three orthogonal
        planes, creating 15 image channels
        \linebreak Model: LeNet CNN
        & Grasp candidates sampled for object models are labeled using force-closure
        property
        \\ \cmidrule[0.01em](){1-4}
        Mahler et al. \cite{mahler2017}       & Two gripper plates visualized in depth images
        & Depth images are cropped and aligned to the gripper
        \linebreak Model: CNN combined with single layer neural networks
        & Grasp candidates sampled for object models are labeled using a variant
        of the robust $ \epsilon $-metric from \cite{WeiszAllen2012}
        \\ \cmidrule[0.08em](){1-4}
    \end{tabularx}
    \caption{\small Recent machine learning approaches to grasp quality prediction. Details of the different
        object-grasp representations and feature extraction techniques can be found in subsection
        \ref{subsub:object_grasp_local}, while data synthesis methods are examined more closely in subsection
        \ref{subsub:data_synthesis}.}
    \label{table:grasp_quality_approaches}
\end{table}

Table \ref{table:grasp_quality_approaches} lists recent and prominent empirical approaches to predicting grasp quality.
Earlier techniques often rely on carefully designed features. Jiang et al.'s \cite{jiang2011} calculate spatial
histograms for 17 different filters of each pixel in each RGB-D input image. These features are linearly combined using
a weight matrix, creating a score value for each pixel. Three such weight matrices are initialized, creating three
pixel-wise score matrices. For each bipedal grasp, the aforementioned rectangle representation is divided into three
regions. For each region, the respective score values are extracted from one of the three score matrices. The final
score for grasp is the sum of all the extracted pixel scores. A Support Vector Machine (SVM) algorithm is used to learn
the weight matrices. Lenz et al \cite{lenz2015} train auto-encoders to extract features from the rectangles
representing grasps in RGB-D images. Weights from these auto-encoders are used to initialize multilayer-perceptron
(MLP) models, which are then trained to predict the probability of success of given grasps.

More recent approaches project local features at contact locations onto 2D representations then apply CNN methods in a
similar manner with RGB images to learn the grasp rating directly \cite{Gualtieri2016,mahler2017,Kappler2015}. These
methods mainly differ in the representations their CNN models take as input, as described in subsection
\ref{subsub:object_grasp_local}. While \cite{Gualtieri2016,Kappler2015} implement a pure LeNet CNN model \cite{Gu2018}
to learn the grasp quality directly, Mahler et al \cite{mahler2017} also include a single-layer fully connected network
which takes the distance between the camera and the gripper as input. The outputs of the CNN and the fully-connected
network are then combined using more fully-connected layers to produce a final grasp success probability prediction.

Methods trying to learn grasp quality are highly dependent on the dataset on which their models are trained. Humans
are still the most versatile and dexterous manipulators known, but data containing human grasp experience is difficult
and costly to collect. Of the approaches listed in table \ref{table:grasp_quality_approaches}, Jiang et al.'s
\cite{jiang2011} train their model on a human-labeled dataset, which only has around 300 data points for both training
and testing. The extended version of this dataset, used by Lenz et al. \cite{lenz2015}, also has only around 1000 data
points. Because of this scarcity, synthesizing data for training grasp evaluation models has become popular in the more
recent approaches \cite{Kappler2015,Gualtieri2016,mahler2017}. A review of data synthesis techniques for grasping will
be included in the next section. The introduction of a new large-scale human grasp experience dataset by Saudabayev et
al. \cite{Saudabayev2018}, however, may open up new possibilities for direct training of grasp evaluation models on
human grasp experience.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating data for grasp success prediction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data synthesis for robot grasping} \label{subsub:data_synthesis}

Most approaches to synthesize data for grasp evaluation models generate training samples from object meshes. Earlier
methods then learn to match detected objects with these object models to find suitable grasps to execute. Notably
Goldfeder et al. \cite{Goldfeder2009CGDB} generate a database of form-closure grasps for 3D object models, in which
each grasp entry is represented by a pregrasp in the form of a two-dimensional eigengrasp \cite{Ciocarlie2009}, and the
final grasp pose. Grasps are sampled from a subspace of each robot hand's degree-of-freedom (DOF) space. The authors
then used a nearest-neighbor algorithm to match each detected object to a model in this database \cite{Goldfeder2011}
to find grasp candidates.

Several grasp synthesis approaches generate perceptual data from object meshes at different viewpoints
\cite{mahler2017,Gualtieri2016,Kappler2015} for constructing a grasp knowledge database. Mahler et al. \cite{mahler2017}
use 3D mesh models from Dex-Net 1.0 \cite{mahler2016}, rescaling them to fit within the gripper's physical constraints.
Stable poses are computed for each object, and poses with a probability of occurrence above a threshold are stored.
For each stable pose, a set of grasps is sampled perpendicular to the table and collision-free. Rejection sampling
is used during grasp generation to ensure coverage of the object surface. Grasps are then ranked and selected using
a trained grasp quality metric. Finally, 2D images are generated from these grasp candidates as training data with the
antipodal axis aligned to the middle row of the image. Figure \ref{fig:dexnet-data-gen} provides a visualization of this
data generation process.

Kappler et al. \cite{Kappler2015} generate data using the \footnoteHref{http://openrave.org/}{OpenRAVE}
\cite{Diankov2010} simulator. Reference grasps are sampled using the geometric strategy available in OpenRAVE, where the
contact points are approach points, and the surface normals are approach vectors. For each approach point, 8 wrist
rotations around the approach vector and two translation offsets are applied, resulting in 16 candidate grasps per
reference grasp. Data points are then generated for each grasp using the local shape representation described in
subsection \ref{subsub:object_grasp_local}. A set of metrics is then applied to evaluate candidate grasps, creating the
data labels.

All these approaches, however, still rely on analytical metrics to label grasp candidates. These metrics, as mentioned
in subsection \ref{sub:overview}, can be unreliable when tested on real systems. As robots can rarely acquire the
comprehensive model of their environment from perceptual sensors, training data for robot learning should reflect what
is normally available to the robot's perception pipeline. As a result, this project will focus on data synthesis
approaches which generate grasp knowledge databases synthesizing and labeling perceptual data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data augmentation}

For learned models to become robust to noise in real-world sensors and to alleviate the scarcity of real data,
augmentation techniques are often applied to available data before training
\cite{Eitel2015,Kappler2015,Gupta2014RGBDFeatures}. As defined in \cite{Gu2018}, data augmentation refer to transforming
existing data ``without altering their natures.'' Which features exactly should be preserved during the augmentation
process depend on the data type.

With the relatively recent popularization of RGB-D sensors, few approaches exist to augment depth data. Eitel et al.
\cite{Eitel2015} sample noise patches of fixed size from real RGB-D data and divide them into five groups depending on
the number of missing depth reading in each patch. To create a final noise pattern, a pair of patches are sampled at
random from two different groups, then randomly added or subtracted with each other and optionally inverted. This
process is repeated until a specific number of patterns is generated. During training, at a predefined probability,
a noise pattern will be randomly selected from this set and applied to the depth sample to create the noised input.
Kappler at al \cite{Kappler2015} introduces noise to the object poses while sampling for reference grasps, before
generating each data point. Specifically, at the grasp approach point, the object is rotated around a random axis, then
a random offset is added to the surface pose. These variations of the object pose are intended to capture the errors in
calculating the surface normals and the surface noise during the reconstruction as described in subsection
\ref{subsub:object_grasp_local}. Gupta et al. \cite{Gupta2014RGBDFeatures} have a simpler approach, only adding a
low-frequency white noise to the disparity images to augment the depth information.

In contrast, many augmentation approaches have been developed for images. Several involve simple geometric
transformations such as mirroring, rotating, shifting \cite{Gu2018} or photometric such as color scaling, contrasting
\cite{Eigen2015}. Hauberg et al. \cite{Hauberg2016Diffeomorphism} propose an elegant method to learn these
transformations using diffeomorphisms. The authors first estimate the transformations for image pairs in each class.
These transformations are then used as observations to build a class-wise statistical model of transformations.
New transformations are then sampled from this model and applied to input images.

\todo{should adversarial approaches be included?}

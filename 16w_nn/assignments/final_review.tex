\documentclass[9pt,t]{beamer}

%-------------------------------------------------
%   THEMES & PACKAGES
%-------------------------------------------------
\usetheme[progressbar=frametitle]{metropolis}
\usepackage[belowskip=-15pt]{caption}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}

%-------------------------------------------------
%   Settings
%-------------------------------------------------
\graphicspath{{../images/}{../../images/}}
\setbeamertemplate{footline}[text line]{%
    \parbox{\linewidth}{\vspace*{-8pt}\insertshorttitle\hfill\insertshortsubtitle\hfill\insertshortauthor\hfill\insertpagenumber}}
\setbeamertemplate{navigation symbols}{}

%-------------------------------------------------
%   TITLE
%-------------------------------------------------
\title{Neural Networks}
\subtitle{Final Review}
\date{\today}
\author{Minh Nguyen}
\titlegraphic{\hfill\includegraphics[height=0.7cm]{h-brs-logo.png}}

%-------------------------------------------------
%   COMMANDS
%-------------------------------------------------
\newcommand{\picEqHereWidth}[2] { %
    \begin{figure}[htp] 
        \centering
        \includegraphics[width=#2]{#1}
    \end{figure}
}
\newcommand{\picHereWidth}[4] { %
    \begin{figure}[htp] %
        \centering
        \includegraphics[width=#4]{#1} %
        \caption{#2} %
        \label{#3}
    \end{figure} %
}
%-------------------------------------------------
%   BEGIN
%-------------------------------------------------
\begin{document}

%-------------------------------------------------
\maketitle

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \begin{alertblock}{ANN definition}
        \begin{itemize}
            \item massively parallel distributed processor
            \item (made up of) simple processing units
            \item (good at) storing experiential knowledge \& making it available for use
            \item resembles the brain
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Neuron definition}
        A neuron is a \textbf{basic info processing unit of a NN}, consisting of:
        \begin{itemize}
            \item set of connecting links (\textbf{weights} or synapses)
            \item an \textbf{adder} function (linear combiner): computes weighted inputs
            \item \textbf{activation function} (squashing function): limits neuron output. Types:
            \begin{itemize}
                \item threshold (McCulloch-Pitts model): step function
                \item piece-wise linear: linear on $(\frac{-1}{2}, \frac{1}{2})$, like step function elsewhere.
                \item sigmoid.
            \end{itemize}
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \picHereWidth{neuron_graph}{Neuron graph}{fig:neuron}{0.7\linewidth}
    \begin{alertblock}{Types of NN architectures}
        \begin{itemize}
            \item single layer feed forward (neurons organized)
            \item multilayer feed forward (acyclic layers)
            \item recurrent (acyclic layers)
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \begin{figure}[htp!]
        \centering
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=.7\linewidth]{mlp.png}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=\linewidth]{rnn.png}
        \end{subfigure}
        \caption{MLP and RNN networks}
    \end{figure}
    \begin{alertblock}{Knowledge rules in ANNs}
        \begin{itemize}
            \item similar inputs from similar classes produce similar representations
            \item items from separated classes should receive different representations
            \item important features should be represented using large number of neurons
            \item prior info and \textbf{invariance} should be built-in to the network
            \begin{itemize}
                \item invariance by structure: $w_{ij} = w_{ji}$
                \item invariance by training: many examples
                \item invariant feature space
            \end{itemize}
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \picHereWidth{reinforce_learn.png}{Reinforcement learning}{fig:rf}{0.5\linewidth}
    \begin{alertblock}{Learning paradigms}
        \begin{itemize}
            \item supervised: build input-output relation known through examples
            \item unsupervised: model properties of inputs
            \item reinforcement learning: learn from outputs -- whether the output is good or bad.
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \begin{alertblock}{Classification vs regression}
        \begin{itemize}
            \item regression outputs continuous values, whereas classification outputs discrete class labels
            \item regression examples: system identification, inverse modeling, controller
            \item classification examples: pattern matching,.
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Similarity between polynomial curve fitting and NN regression}
        \begin{itemize}
            \item Similar formulation: polynoms $y = \sum_{j = 0}^{M} w_j x_j$; NN $y = f (\textbf{w}, \textbf{x})$
            \item Same error criterion $E = \sum_{p = 1}^{P}(y^p - t^p) ^2$
            \item Min error solution: polynoms -- quadratic in $\textbf{w}$, min(E) is solution of a set of linear equations; NN -- nonlinear in $\textbf{w}$, solve for local minima
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Learning vs. generalization}
        \begin{itemize}
            \item Learning: find parameters $\textbf{w}$ to minimize error $ E $ given a set of examples
            \item Generalization: assign output value $ y $ to a new input $ x $ given $ \textbf{w} $
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Artificial Neural Networks}
    \begin{alertblock}{Overfitting}
        NN are \textbf{universal approximation models} with (relatively) \textbf{low complexity} $ \Rightarrow $ prone to overfitting
    \end{alertblock}
    \begin{alertblock}{model complexity}
        \begin{itemize}
            \item hard to compromise between performance and complexity $ \Rightarrow  $ control \emph{effective} complexity, use new error $ \tilde{E} = E + \lambda \Omega $ with penalty term:
            \[ \Omega = \frac{1}{2} \int \left(\frac{d^2 y}{x^2}\right)^2 dx \]
            \item many possible forms of $ \Omega $ and $ \lambda $ is hard to adjust.
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Curse of dimensionality}
        \begin{itemize}
            \item More features lead to worse performance
            \item For few data samples dimension should be low
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \picHereWidth{learning.png}{Learning process}{fig:learning}{0.6\linewidth}
    \begin{alertblock}{Perceptron learning for classification}
        \begin{figure}[htp!]
            \centering
            \begin{subfigure}{.3\textwidth}
                \centering
                \includegraphics[width=\linewidth]{perceptron_learning1.png}
            \end{subfigure}%
            \begin{subfigure}{.3\textwidth}
                \centering
                \includegraphics[width=\linewidth]{perceptron_learning2.png}
            \end{subfigure}
            \begin{subfigure}{.3\textwidth}
                \centering
                \includegraphics[width=\linewidth]{perceptron_learning3.png}
            \end{subfigure}
        \end{figure}
        \picEqHereWidth{perceptron_learning_eq.png}{0.3\linewidth}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{alertblock}{Adaline learning for regression}
        Adaline (\textbf{Ada}ptive \textbf{Lin}ear \textbf{E}lement) architecture:
        \begin{itemize}
            \item perceptron with linear activation function
            \item learning: error correction derived from LMS:
            \[ w(n+1) = w(n) - \eta \nabla E(w(n)) \approx w(n) + \eta x(w e(n)) \]
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Fixed-increment learning algorithm}
        \begin{enumerate}
            \item initialization: $ n = 1, (n) = 0 $
            \item activate perceptron by applying input $ x(n) $
            \item compute output: $ y(n) = step(w^T(n) x(n)) $
            \item adapt weight vector: if $ d(n) \neq y(n) $ $ \Rightarrow w(n+1) = w(n) + \eta e(n) x(n) $,
            where $ e(n) = +1 $ for $ \textbf{X}(n) \in C_1 $ and $ e(n) = -1 $ for $ \textbf{X}(n) \in C_2 $
            \item continuation: increment $n$ and go to activation (step 2)
        \end{enumerate}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{alertblock}{Convergence theorem}
        Proof that perceptron algorithm converges
        \begin{itemize}
            \item Let $ \mathbf{w_0} $ be such that $ \mathbf{w_0^T x}(n) > 0, ~ \forall \mathbf{x}(n) \in C_1 $
            \item Let $ \alpha = \min(\mathbf{w_0^T x}(n)), ~ \forall \mathbf{x}(n) \in C_1 \Rightarrow $ Cauchy-Schwarz inequality: $ ||\mathbf{w}(k+1)||^2 \geq \cfrac{k^2 \alpha^2}{||\textbf{w}_0||^2} $
            \item Let $ \beta  = \max(||\textbf{x}(n)||^2), ~ \forall \mathbf{x}(n) \in C_1  \Rightarrow ||\mathbf{w}(k+1)||^2 \leq k \beta $
            \item To satisfied both above conditions, perceptron algorithm must terminate in at most $ n_{max} $ iterations: $ n_{max} = \cfrac{\beta ||\textbf{w}_0||^2}{\alpha^2} $
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{Perceptron limitation}
        \begin{itemize}
            \item can only model linearly separable functions.
            \item can model AND, OR, COMPLEMENT but not XOR.
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{alertblock}{Adaline vs. Perceptron}
        \begin{itemize}
            \item Model: Adaline uses linear, perceptron uses nonlinear activation functions.
            \item Learning: Adaline's continuous, perceptron has finite number of iterations.
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{ANN learning definition}
        \begin{itemize}
            \item process by which \textbf{free parameters} of a NN \textbf{are adapted} in a desired way
            \item through a \textbf{process of stimulation} by the environment
            \item type of learning is determined by how the parameter update are performed.
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{1st learning rule -- Error Correction}
        \begin{itemize}
            \item Optimize cost function (instantaneous at time $ n $, local at output node $ k $): $ \mathcal{E}(n) = \frac{1}{2} e_k^2(n) $, where $ e_k(n) = d_k(n) - y_k(n) $
            \item Widrow-Hoff rule (delta rule) for weight update: $ w_{kj}(n+1) = w_{kj}(n) + \eta \Delta w_{kj}(n) = w_{kj}(n) + \eta e_k(n) x_j(n) $
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{wrapfigure}{r}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{knn_algorithm.png} \\\hfill\\
        \includegraphics[width=0.8\linewidth]{hebbs.png}
    \end{wrapfigure}
    \begin{alertblock}{2nd learning rule -- Memory based}
        \begin{itemize}
            \item Nearest Neighbor definition: Given $ L = \{x_1, x_2,...,x_N\} $ and $ x_{test} \notin L $, $ x' \in L $ is nearest neighbor to $ x{test} \in L \iff \min_i d(x_i, x_{test}) = d(x', x_{test}) $
            \item k-NN algorithm (right).
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{3rd learning rule -- Hebbian}
        \begin{itemize}
            \item simultaneous~activation~of~2~neurons on either~side~of~a~synapse~$ \rightarrow $~synaptic weight increases
            \item asynchronous~activation~of~2~neurons~$ \rightarrow $~synaptic weight decreases
        \end{itemize}
    \end{alertblock}

\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{alertblock}{3rd learning rule -- Hebbian}
        \begin{itemize}
            \item 4 mechanisms: time-dependent, local, interactive, conjunctional/correlational.
            \item Covariance hypothesis: $ \Delta W_{kj}(n) = \eta (y_k - \bar{y})(x_j - \bar{x}) $
        \end{itemize}
    \end{alertblock}
    \begin{alertblock}{4th learning rule -- Competitive learning}
        \picEqHereWidth{competitive_learning}{0.7\linewidth}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Single layer learning}
    \begin{alertblock}{5th learning rule -- Boltzmann}
        \begin{itemize}
            \item Recurrent ANN with \textbf{binary neurons}.
            \item Operate by flipping -- probability of turning on:
            \picEqHereWidth{boltzmann_eq.png}{0.5\linewidth}
            \item Modes of operation: clamped and free running conditions:
            \picEqHereWidth{boltzmann_eq2.png}{0.4\linewidth}
            where $ \rho^+_{kj} $ and $ \rho^-_{kj} $ are correlation in clamped and free running states.
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Statistical Learning Theory}
    \begin{alertblock}{VC-Dimension}
        \begin{itemize}
            \item Given learning machine $ \mathbf{f} $ with vector of adjustable parameters $ \alpha $. Define probability of misclassification:
            \picEqHereWidth{prob_misclass.png}{0.6\linewidth}
            \item Define fraction of training set misclassified ($ R $ is number of training set data points):
            \picEqHereWidth{prob_train_misclass.png}{0.7\linewidth}
            \item Let $ h $ be VC dimension of $ \mathbf{f} $. With probability $ 1 - \eta $ (Vapnik):
            \picEqHereWidth{prob_misclass_vapnik}{0.8\linewidth}
            $ \rightarrow $ allow estimating future error based on training error and VC dimension of $ \mathbf{f} $
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Statistical Learning Theory}
    \begin{alertblock}{Shattering}
        Machine $ f $ can shatter set of point $ x_1, x_2, ..., x_r $ iff $ \forall $ training set $ (x_1, y_1), (x_2, y_2), ... , (x_r, y_r) $, $ \exists \alpha $ with no training error ($ 2^r $ possible sets).
    \end{alertblock}
    \begin{alertblock}{VC dimension}
        \begin{itemize}
            \item Terms: instance space $ X $ , hypothesis $ h \in \mathcal{H} $
            \item $ \mathcal{H} $ shatters $ A \subseteq X $ if $ \exists h \in \mathcal{H} $ that separate negative from positive examples (for any possible labeling of A).
            \item Definition: largest finite subset $ A \subseteq X $ than can be shattered by $ \mathcal{H} $
            \item If VC dimension is $ m $
            \begin{itemize}
                \item $ \exists $ at least 1 set of $ m $ points $ \in X $ that can be shattered
                \item not every set of $ m $ points can necessarily be shattered
            \end{itemize}
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
\begin{frame}{Learning as Nonlinear Optimization}
    \begin{alertblock}{Shattering}
        Machine $ f $ can shatter set of point $ x_1, x_2, ..., x_r $ iff $ \forall $ training set $ (x_1, y_1), (x_2, y_2), ... , (x_r, y_r) $, $ \exists \alpha $ with no training error ($ 2^r $ possible sets).
    \end{alertblock}
    \begin{alertblock}{VC dimension}
        \begin{itemize}
            \item Terms: instance space $ X $ , hypothesis $ h \in \mathcal{H} $
            \item $ \mathcal{H} $ shatters $ A \subseteq X $ if $ \exists h \in \mathcal{H} $ that separate negative from positive examples (for any possible labeling of A).
            \item Definition: largest finite subset $ A \subseteq X $ than can be shattered by $ \mathcal{H} $
            \item If VC dimension is $ m $
            \begin{itemize}
                \item $ \exists $ at least 1 set of $ m $ points $ \in X $ that can be shattered
                \item not every set of $ m $ points can necessarily be shattered
            \end{itemize}
        \end{itemize}
    \end{alertblock}
\end{frame}

%-------------------------------------------------
%   END
%-------------------------------------------------
\end{document}

% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Inbook{Bo2013,
  Title                    = {Unsupervised Feature Learning for RGB-D Based Object Recognition},
  Author                   = {Bo, Liefeng
and Ren, Xiaofeng
and Fox, Dieter},
  Booktitle                = {Experimental Robotics: The 13th International Symposium on Experimental Robotics},
  Date                     = {2013},
  Year                     = {2013},
  Doi                      = {10.1007/978-3-319-00065-7\_27},
  Editor                   = {Desai, Jaydev P.
and Dudek, Gregory
and Khatib, Oussama
and Kumar, Vijay},
  ISBN                     = {978-3-319-00065-7},
  Pages                    = {387--402},
  Publisher                = {Springer International Publishing},

  Abstract                 = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
  Address                  = {Heidelberg}
}

@Inproceedings{Bohg2011MindTheGap,
  Title                    = {Mind the gap - robotic grasping under incomplete observation},
  Author                   = {J. Bohg and M. Johnson-Roberson and B. León and J. Felip and X. Gratal and N. Bergström and D. Kragic and A. Morales},
  Booktitle                = {2011 IEEE International Conference on Robotics and Automation},
  Date                     = {2011-05},
  Editor                   = {Yuan F. Zheng},
  Year                     = {2011},
  Doi                      = {10.1109/ICRA.2011.5980354},
  Month                    = {May},
  Pages                    = {686-693},

  Abstract                 = {We consider the problem of grasp and manipulation planning when the state of the world is only partially observable. Specifically, we address the task of picking up unknown objects from a table top. The proposed approach to object shape prediction aims at closing the knowledge gaps in the robot's understanding of the world. A completed state estimate of the environment can then be provided to a simulator in which stable grasps and collision-free movements are planned. The proposed approach is based on the observation that many objects commonly in use in a service robotic scenario possess symmetries. We search for the optimal parameters of these symmetries given visibility constraints. Once found, the point cloud is completed and a surface mesh reconstructed. Quantitative experiments show that the predictions are valid approximations of the real object shape. By demonstrating the approach on two very different robotic platforms its generality is emphasized.},
  ISSN                     = {1050-4729},
  Keywords                 = {mesh generation;robots;collision-free movements;gap robotic grasping;incomplete observation;manipulation planning;mesh reconstruction;object shape prediction;Approximation methods;Grasping;Image reconstruction;Planning;Robots;Shape;Surface reconstruction}
}

@Article{Bohg2014,
  Title                    = {Data-Driven Grasp Synthesis: A Survey},
  Author                   = {J. Bohg and A. Morales and T. Asfour and D. Kragic},
  Date                     = {2014-04-02},
  Journaltitle             = {IEEE Transactions on Robotics},
  Year                     = {2014},
  Doi                      = {10.1109/TRO.2013.2289018},
  ISSN                     = {1552-3098},
  Month                    = {April},
  Number                   = {2},
  Pages                    = {289-309},
  Volume                   = {30},

  Abstract                 = {We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps. We divide the approaches into three groups based on whether they synthesize grasps for known, familiar, or unknown objects. This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique. In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation. In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects. Finally, for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps. Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping. We also draw a parallel to the classical approaches that rely on analytic formulations.},
  Journal                  = {IEEE Transactions on Robotics},
  Keywords                 = {feature extraction;grippers;image matching;object recognition;pose estimation;sampling methods;candidate grasp ranking;candidate grasp sampling;common object representations;data-driven grasp synthesis technique;feature extraction;object recognition;perceptual processes;pose estimation;robot grasping;similarity matching;Databases;Feature extraction;Grasping;Measurement;Robot sensing systems;Grasp planning;grasp synthesis;object grasping and manipulation;object recognition and classification;visual perception;visual representations}
}

@Article{Ciocarlie2009,
  Title                    = {Hand Posture Subspaces for Dexterous Robotic Grasping},
  Author                   = {Matei T. Ciocarlie and Peter K. Allen},
  Date                     = {2009-06-26},
  Journaltitle             = {The International Journal of Robotics Research},
  Year                     = {2009},
  Doi                      = {10.1177/0278364909105606},
  Eprint                   = {https://doi.org/10.1177/0278364909105606},
  Number                   = {7},
  Pages                    = {851-867},
  Volume                   = {28},

  Abstract                 = {In this paper we focus on the concept of low-dimensional posture subspaces for artificial hands. We begin by discussing the applicability of a hand configuration subspace to the problem of automated grasp synthesis; our results show that low-dimensional optimization can be instrumental in deriving effective pre-grasp shapes for a number of complex robotic hands. We then show that the computational advantages of using a reduced dimensionality framework enable it to serve as an interface between the human and automated components of an interactive grasping system. We present an on-line grasp planner that allows a human operator to perform dexterous grasping tasks using an artificial hand. In order to achieve the computational rates required for effective user interaction, grasp planning is performed in a hand posture subspace of highly reduced dimensionality. The system also uses real-time input provided by the operator, further simplifying the search for stable grasps to the point where solutions can be found at interactive rates. We demonstrate our approach on a number of different hand models and target objects, in both real and virtual environments. },
  Journal                  = {The International Journal of Robotics Research}
}

@Inproceedings{Detry2009,
  Title                    = {Learning object-specific grasp affordance densities},
  Author                   = {R. Detry and E. Baseski and M. Popovic and Y. Touati and N. Kruger and O. Kroemer and J. Peters and J. Piater},
  Booktitle                = {2009 IEEE 8th International Conference on Development and Learning},
  Date                     = {2009-06},
  Editor                   = {Zhengyou Zhang},
  Year                     = {2009},
  Doi                      = {10.1109/DEVLRN.2009.5175520},
  Month                    = {June},
  Pages                    = {1-7},

  Abstract                 = {This paper addresses the issue of learning and representing object grasp affordances, i.e. object-gripper relative configurations that lead to successful grasps. The purpose of grasp affordances is to organize and store the whole knowledge that an agent has about the grasping of an object, in order to facilitate reasoning on grasping solutions and their achievability. The affordance representation consists in a continuous probability density function defined on the 6D gripper pose space-3D position and orientation-, within an object-relative reference frame. Grasp affordances are initially learned from various sources, e.g. from imitation or from visual cues, leading to grasp hypothesis densities. Grasp densities are attached to a learned 3D visual object model, and pose estimation of the visual model allows a robotic agent to execute samples from a grasp hypothesis density under various object poses. Grasp outcomes are used to learn grasp empirical densities, i.e. grasps that have been confirmed through experience. We show the result of learning grasp hypothesis densities from both imitation and visual cues, and present grasp empirical densities learned from physical experience by a robot.},
  Keywords                 = {intelligent robots;learning systems;manipulators;multi-agent systems;object recognition;pose estimation;robot vision;3D orientation;3D position;6D gripper pose space;affordance representation;continuous probability density function;grasp empirical densities;grasp hypothesis densities;grasping solutions;learned 3D visual object model;object grasp affordances;object-gripper relative configurations;object-relative reference frame;pose estimation;robotic agent;Autonomous agents;Biological system modeling;Density functional theory;Encoding;Grippers;Humans;Kernel;Probability density function;Robots;Solid modeling}
}

@Inproceedings{Detry2012,
  Title                    = {Generalizing grasps across partly similar objects},
  Author                   = {R. Detry and C. H. Ek and M. Madry and J. Piater and D. Kragic},
  Booktitle                = {2012 IEEE International Conference on Robotics and Automation},
  Date                     = {2012-05},
  Editor                   = {Antonio Bicchi},
  Year                     = {2012},
  Doi                      = {10.1109/ICRA.2012.6224992},
  Month                    = {May},
  Pages                    = {3791-3797},

  Abstract                 = {The paper starts by reviewing the challenges associated to grasp planning, and previous work on robot grasping. Our review emphasizes the importance of agents that generalize grasping strategies across objects, and that are able to transfer these strategies to novel objects. In the rest of the paper, we then devise a novel approach to the grasp transfer problem, where generalization is achieved by learning, from a set of grasp examples, a dictionary of object parts by which objects are often grasped. We detail the application of dimensionality reduction and unsupervised clustering algorithms to the end of identifying the size and shape of parts that often predict the application of a grasp. The learned dictionary allows our agent to grasp novel objects which share a part with previously seen objects, by matching the learned parts to the current view of the new object, and selecting the grasp associated to the best-fitting part. We present and discuss a proof-of-concept experiment in which a dictionary is learned from a set of synthetic grasp examples. While prior work in this area focused primarily on shape analysis (parts identified, e.g., through visual clustering, or salient structure analysis), the key aspect of this work is the emergence of parts from both object shape and grasp examples. As a result, parts intrinsically encode the intention of executing a grasp.},
  ISSN                     = {1050-4729},
  Keywords                 = {image matching;manipulators;robot vision;unsupervised learning;dictionary;dimensionality reduction;generalize grasping strategies;grasp planning;grasp transfer problem;learning;object matching;object parts;proof-of-concept experiment;robot grasping;similar objects;unsupervised clustering algorithms;Covariance matrix;Databases;Dictionaries;Grasping;Grippers;Planning;Shape}
}

@Phdthesis{Diankov2010,
  Title                    = {Automated Construction of Robotic Manipulation Programs},
  Author                   = {Rosen Diankov},
  Date                     = {2010-08-26},
  Institution              = {Carnegie Mellon University},
  Year                     = {2010}
}

@Inproceedings{Eigen2015,
  Title                    = {Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture},
  Author                   = {D. Eigen and R. Fergus},
  Booktitle                = {2015 IEEE International Conference on Computer Vision (ICCV)},
  Date                     = {2015-12},
  Editor                   = {Katsushi Ikeuchi and Christoph Schnörr and Josef Sivic and René Vidal},
  Year                     = {2015},
  Doi                      = {10.1109/ICCV.2015.304},
  Month                    = {Dec},
  Pages                    = {2650-2658},

  Abstract                 = {In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.},
  Keywords                 = {computer vision;convolution;prediction theory;computer vision tasks;depth prediction;multiscale convolutional architecture;multiscale convolutional network;semantic labeling;surface normal estimation;Adaptation models;Estimation;Image segmentation;Labeling;Semantics;Spatial resolution}
}

@Inproceedings{Eitel2015,
  Title                    = {Multimodal deep learning for robust RGB-D object recognition},
  Author                   = {A. Eitel and J. T. Springenberg and L. Spinello and M. Riedmiller and W. Burgard},
  Booktitle                = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Date                     = {2015-09},
  Editor                   = {Wolfram Burgard},
  Year                     = {2015},
  Doi                      = {10.1109/IROS.2015.7353446},
  Month                    = {Sept},
  Pages                    = {681-687},

  Abstract                 = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.},
  Keywords                 = {feedforward neural nets;image colour analysis;image fusion;learning (artificial intelligence);object recognition;robot vision;CNN;RGB-D architecture;RGB-D object dataset;RGB-D real-world noisy settings;accurate learning;convolutional neural networks;data augmentation scheme;fusion network;imperfect sensor data;multimodal deep learning;multistage training methodology;real-world robotics applications;real-world robotics tasks;realistic noise patterns;robust RGB-D object recognition;robust learning;Feature extraction;Image coding;Object recognition;Robot sensing systems;Robustness;Streaming media;Training}
}

@Inproceedings{Fawzi2016,
  Title                    = {Adaptive data augmentation for image classification},
  Author                   = {A. Fawzi and H. Samulowitz and D. Turaga and P. Frossard},
  Booktitle                = {2016 IEEE International Conference on Image Processing (ICIP)},
  Date                     = {2016-09},
  Editor                   = {Lina Karam},
  Year                     = {2016},
  Doi                      = {10.1109/ICIP.2016.7533048},
  Month                    = {Sept},
  Pages                    = {3688-3692},

  Abstract                 = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
  Keywords                 = {data handling;gradient methods;image classification;linear programming;neural nets;stochastic processes;adaptive data augmentation;deep neural networks;image classification;linear program sequence;stochastic gradient descent algorithm;trust-region optimization strategy;Approximation algorithms;Neural networks;Optimization;Robustness;Training;Training data;Transforms;Data augmentation;image robustness;transformation invariance;trust-region optimization}
}

@Inproceedings{FerrariCanny1992,
  Title                    = {Planning optimal grasps},
  Author                   = {C. Ferrari and J. Canny},
  Booktitle                = {Proceedings 1992 IEEE International Conference on Robotics and Automation},
  Date                     = {1992-05},
  Editor                   = {Giuseppe Menga

Giuseppe Menga

Giuseppe Menga

Giuseppe Menga

Giuseppe Menga},
  Year                     = {1992},
  Doi                      = {10.1109/ROBOT.1992.219918},
  Month                    = {May},
  Pages                    = {2290-2295 vol.3},

  Abstract                 = {The authors address the problem of planning optimal grasps. Two general optimality criteria that consider the total finger force and the maximum finger force are introduced and discussed. Their formalization using various metrics on a space of generalized forces is detailed. The geometric interpretation of the two criteria leads to an efficient planning algorithm. An example of its use in a robotic environment equipped with two-jaw and three-jaw is described.<<ETX>>},
  Keywords                 = {computational geometry;force control;manipulators;optimisation;planning (artificial intelligence);optimal grasp planning;robotics;manipulators;optimality criteria;total finger force;maximum finger force;geometric interpretation;Grippers;Robots;Fingers;Extraterrestrial measurements;Orbital robotics;Actuators;Manipulators;Grasping;Assembly systems;Computer science}
}

@Article{Goldfeder2011,
  Title                    = {Data-driven grasping},
  Author                   = {Goldfeder, Corey
and Allen, Peter K.},
  Date                     = {2011-07-01},
  Journaltitle             = {Autonomous Robots},
  Year                     = {2011},
  Doi                      = {10.1007/s10514-011-9228-1},
  ISSN                     = {1573-7527},
  Month                    = {Jul},
  Number                   = {1},
  Pages                    = {1--20},
  Volume                   = {31},

  Abstract                 = {This paper propose a novel framework for a data driven grasp planner that indexes partial sensor data into a database of 3D models with known grasps and transfers grasps from those models to novel objects. We show how to construct such a database and also demonstrate multiple methods for matching into it, aligning the matched models with the known sensor data of the object to be grasped, and selecting an appropriate grasp to use. Our approach is experimentally validated in both simulated trials and trials with robots.},
  Day                      = {01},
  Journal                  = {Autonomous Robots}
}

@Inproceedings{Goldfeder2009CGDB,
  Title                    = {The Columbia grasp database},
  Author                   = {C. Goldfeder and M. Ciocarlie and Hao Dang and P. K. Allen},
  Booktitle                = {2009 IEEE International Conference on Robotics and Automation},
  Date                     = {2009-05},
  Editor                   = {Antonio Bicchi},
  Year                     = {2009},
  Doi                      = {10.1109/ROBOT.2009.5152709},
  Month                    = {May},
  Pages                    = {1710-1716},

  Abstract                 = {Collecting grasp data for learning and benchmarking purposes is very expensive. It would be helpful to have a standard database of graspable objects, along with a set of stable grasps for each object, but no such database exists. In this work we show how to automate the construction of a database consisting of several hands, thousands of objects, and hundreds of thousands of grasps. Using this database, we demonstrate a novel grasp planning algorithm that exploits geometric similarity between a 3D model and the objects in the database to synthesize form closure grasps. Our contributions are this algorithm, and the database itself, which we are releasing to the community as a tool for both grasp planning and benchmarking.},
  ISSN                     = {1050-4729},
  Keywords                 = {database management systems;dexterous manipulators;intelligent robots;learning (artificial intelligence);planning (artificial intelligence);3D model;Columbia grasp database;dexterous robotic grasping;geometric similarity;grasp planning algorithm;machine learning;robot hand;Computational geometry;Computational modeling;Data gloves;Humans;Large-scale systems;Robotics and automation;Robots;Solid modeling;Spatial databases;Training data}
}

@Incollection{Goodfellow2014GAN,
  Title                    = {Generative Adversarial Nets},
  Author                   = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Date                     = {2014-12},
  Editor                   = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  Year                     = {2014},
  Pages                    = {2672--2680},
  Publisher                = {Curran Associates, Inc.},
  Url                      = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@Article{Gu2018,
  Title                    = {Recent advances in convolutional neural networks},
  Author                   = {Jiuxiang Gu and Zhenhua Wang and Jason Kuen and Lianyang Ma and Amir Shahroudy and Bing Shuai and Ting Liu and Xingxing Wang and Gang Wang and Jianfei Cai and Tsuhan Chen},
  Date                     = {2018-05},
  Journaltitle             = {Pattern Recognition},
  Year                     = {2018},
  Doi                      = {10.1016/j.patcog.2017.10.013},
  ISSN                     = {0031-3203},
  Pages                    = {354 - 377},
  Volume                   = {77},

  Abstract                 = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
  Journal                  = {Pattern Recognition},
  Keywords                 = {Convolutional neural network, Deep learning}
}

@Inproceedings{Gualtieri2016,
  Title                    = {High precision grasp pose detection in dense clutter},
  Author                   = {Marcus Gualtieri and Andreas ten Pas and Kate Saenko and Robert Platt},
  Booktitle                = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Date                     = {2016-10},
  Editor                   = {Wolfram Burgard et al.},
  Year                     = {2016},
  Doi                      = {10.1109/IROS.2016.7759114},
  Month                    = {Oct},
  Pages                    = {598-605},

  Abstract                 = {This paper considers the problem of grasp pose detection in point clouds. We follow a general algorithmic structure that first generates a large set of 6-DOF grasp candidates and then classifies each of them as a good or a bad grasp. Our focus in this paper is on improving the second step by using depth sensor scans from large online datasets to train a convolutional neural network. We propose two new representations of grasp candidates, and we quantify the effect of using prior knowledge of two forms: instance or category knowledge of the object to be grasped, and pretraining the network on simulated depth data obtained from idealized CAD models. Our analysis shows that a more informative grasp candidate representation as well as pretraining and prior knowledge significantly improve grasp detection. We evaluate our approach on a Baxter Research Robot and demonstrate an average grasp success rate of 93\% in dense clutter. This is a 20\% improvement compared to our prior work.},
  Keywords                 = {image classification;image sensors;learning (artificial intelligence);motion control;neurocontrollers;object detection;robot vision;Baxter Research Robot;CAD models;category knowledge;computer aided design models;convolutional neural network training;depth sensor scan;grasp classification;high precision grasp pose detection;instance knowledge;point clouds;Geometry;Grasping;Grippers;Robot sensing systems;Solid modeling;Three-dimensional displays}
}

@Inproceedings{Gupta2014RGBDFeatures,
  Title                    = {Learning Rich Features from RGB-D Images for Object Detection and Segmentation},
  Author                   = {Gupta, Saurabh
and Girshick, Ross
and Arbel{\'a}ez, Pablo
and Malik, Jitendra},
  Booktitle                = {Computer Vision -- ECCV 2014},
  Date                     = {2014},
  Editor                   = {Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne},
  Year                     = {2014},
  ISBN                     = {978-3-319-10584-0},
  Pages                    = {345--360},
  Publisher                = {Springer International Publishing},

  Abstract                 = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3{\%}, which is a 56{\%} relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24{\%} relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
  Address                  = {Cham}
}

@Inproceedings{Hauberg2016Diffeomorphism,
  Title                    = {Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation},
  Author                   = {Søren Hauberg and Oren Freifeld and Anders Boesen Lindbo Larsen and John Fisher and Lars Hansen},
  Booktitle                = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  Date                     = {2016-05},
  Editor                   = {Arthur Gretton and Christian C. Robert},
  Year                     = {2016},
  Month                    = {09--11 May},
  Pages                    = {342--350},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Url                      = {http://proceedings.mlr.press/v51/hauberg16.html},
  Volume                   = {51},

  Abstract                 = {Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.},
  Address                  = {Cadiz, Spain},
  File                     = {hauberg16.pdf:http\://proceedings.mlr.press/v51/hauberg16.pdf:PDF}
}

@Inproceedings{jiang2011,
  Title                    = {Efficient grasping from RGBD images: Learning using a new rectangle representation},
  Author                   = {Yun Jiang and S. Moseson and A. Saxena},
  Booktitle                = {IEEE International Conference on Robotics and Automation (ICRA)},
  Date                     = {2011-05-01},
  Editor                   = {Antonio Bicchi},
  Year                     = {2011},
  Doi                      = {10.1109/ICRA.2011.5980145},
  Month                    = {May},
  Pages                    = {3304-3311},

  Abstract                 = {Given an image and an aligned depth map of an object, our goal is to estimate the full 7-dimensional gripper configuration-its 3D location, 3D orientation and the gripper opening width. Recently, learning algorithms have been successfully applied to grasp novel objects-ones not seen by the robot before. While these approaches use low-dimensional representations such as a `grasping point' or a `pair of points' that are perhaps easier to learn, they only partly represent the gripper configuration and hence are sub-optimal. We propose to learn a new `grasping rectangle' representation: an oriented rectangle in the image plane. It takes into account the location, the orientation as well as the gripper opening width. However, inference with such a representation is computationally expensive. In this work, we present a two step process in which the first step prunes the search space efficiently using certain features that are fast to compute. For the remaining few cases, the second step uses advanced features to accurately select a good grasp. In our extensive experiments, we show that our robot successfully uses our algorithm to pick up a variety of novel objects.},
  ISSN                     = {1050-4729},
  Keywords                 = {grippers;image representation;robot vision;solid modelling;3D location;3D oriented rectangle;7-dimensional gripper configuration;RGBD image grasping rectangle;gripper opening width;image plane;learning algorithm;low-dimensional rectangle representation;search space;Complexity theory;Grasping;Grippers;Histograms;Image edge detection;Robots;Three dimensional displays}
}

@Inproceedings{Kappler2015,
  Title                    = {Leveraging big data for grasp planning},
  Author                   = {D. Kappler and J. Bohg and S. Schaal},
  Booktitle                = {IEEE International Conference on Robotics and Automation (ICRA)},
  Date                     = {2015-05},
  Editor                   = {Allison Okamura},
  Year                     = {2015},
  Doi                      = {10.1109/ICRA.2015.7139793},
  Month                    = {May},
  Pages                    = {4304-4311},

  Abstract                 = {We propose a new large-scale database containing grasps that are applied to a large set of objects from numerous categories. These grasps are generated in simulation and are annotated with different grasp stability metrics. We use a descriptive and efficient representation of the local object shape at which each grasp is applied. Given this data, we present a two-fold analysis: (i) We use crowdsourcing to analyze the correlation of the metrics with grasp success as predicted by humans. The results show that the metric based on physics simulation is a more consistent predictor for grasp success than the standard υ-metric. The results also support the hypothesis that human labels are not required for good ground truth grasp data. Instead the physics-metric can be used to generate datasets in simulation that may then be used to bootstrap learning in the real world. (ii) We apply a deep learning method and show that it can better leverage the large-scale database for prediction of grasp success compared to logistic regression. Furthermore, the results suggest that labels based on the physics-metric are less noisy than those from the υ-metric and therefore lead to a better classification performance.},
  ISSN                     = {1050-4729},
  Keywords                 = {Big Data;control engineering computing;database management systems;grippers;planning (artificial intelligence);Big Data;grasp planning;large-scale database;learning method;logistic regression;physics simulation;physics-metric;Databases;Noise measurement;Robots;Shape;Stability analysis;Three-dimensional displays}
}

@Article{lenz2015,
  Title                    = {Deep learning for detecting robotic grasps},
  Author                   = {Ian Lenz and Honglak Lee and Ashutosh Saxena},
  Date                     = {2015-03-16},
  Journaltitle             = {The International Journal of Robotics Research},
  Year                     = {2015},
  Doi                      = {10.1177/0278364914549607},
  Eprint                   = { 
 https://doi.org/10.1177/0278364914549607
 
},
  Number                   = {4-5},
  Pages                    = {705-724},
  Volume                   = {34},

  Abstract                 = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
  Journal                  = {The International Journal of Robotics Research}
}

@Inproceedings{Liu2016SSD,
  Title                    = {SSD: Single Shot MultiBox Detector},
  Author                   = {Liu, Wei
and Anguelov, Dragomir
and Erhan, Dumitru
and Szegedy, Christian
and Reed, Scott
and Fu, Cheng-Yang
and Berg, Alexander C.},
  Booktitle                = {Computer Vision -- ECCV 2016},
  Date                     = {2016-09-17},
  Editor                   = {Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
  Year                     = {2016},
  ISBN                     = {978-3-319-46448-0},
  Pages                    = {21--37},
  Publisher                = {Springer International Publishing},

  Abstract                 = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$} 300 {\texttimes} 300 input, SSD achieves 74.3 {\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$} 512 {\texttimes} 512 input, SSD achieves 76.9 {\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  Address                  = {Cham}
}

@Article{mahler2017,
  Title                    = {Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics},
  Author                   = {Jeffrey Mahler and
 Jacky Liang and
 Sherdil Niyaz and
 Michael Laskey and
 Richard Doan and
 Xinyu Liu and
 Juan Aparicio Ojea and
 Ken Goldberg},
  Date                     = {2017-03-01},
  Journaltitle             = {Robotics: Science and Systems (RSS)},
  Year                     = {2017},
  Eprint                   = {1703.09312},
  Url                      = {http://arxiv.org/abs/1703.09312},
  Volume                   = {abs/1703.09312},

  Abstract                 = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net.},
  Archiveprefix            = {arXiv},
  Journal                  = {CoRR},
  Timestamp                = {Wed, 07 Jun 2017 14:41:15 +0200}
}

@Inproceedings{mahler2016,
  Title                    = {Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards},
  Author                   = {Mahler, Jeffrey and Pokorny, Florian T and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kr{\"o}ger, Torsten and Kuffner, James and Goldberg, Ken},
  Booktitle                = {IEEE International Conference on Robotics and Automation (ICRA)},
  Date                     = {2016-05},
  Editor                   = {Allison Okamura},
  Year                     = {2016},
  Organization             = {IEEE},
  Pages                    = {1957--1964},

  Abstract                 = {This paper presents the Dexterity Network (Dex-Net) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a Multi- Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/.}
}

@Misc{robocupRulebook2018,
  Title                    = {RoboCup@Home 2018: Rules and Regulations},
  Author                   = {Matamoros, Mauricio AND Rascon, Caleb AND Hart, Justin AND
Holz, Dirk AND van Beek, Loy},
  Date                     = {2018-06-04},
  Year                     = {2018},
  HowPublished             = {\url{http://www.robocupathome.org/rules/2018_rulebook.pdf}}
}

@Book{Murray1994,
  Title                    = {A Mathematical Introduction to Robotic Manipulation},
  Author                   = {Murray, Richard M. and Sastry, S. Shankar and Zexiang, Li},
  Date                     = {1994},
  Year                     = {1994},
  Edition                  = {1st},
  ISBN                     = {0849379814},
  Publisher                = {CRC Press, Inc.},

  Address                  = {Boca Raton, FL, USA}
}

@Techreport{Padalkar2018,
  Title                    = {Dynamic Motion Primitives},
  Author                   = {Abhishek Padalkar},
  Date                     = {2018-08},
  Institution              = {Hochschule Bonn-Rhein-Sieg},
  Year                     = {2018},
  Pubstate                 = {Unpublished}
}

@Article{Pas2017,
  Title                    = {Grasp Pose Detection in Point Clouds},
  Author                   = {Andreas ten Pas and Marcus Gualtieri and Kate Saenko and Robert Platt},
  Date                     = {2017-10-30},
  Journaltitle             = {The International Journal of Robotics Research},
  Year                     = {2017},
  Doi                      = {10.1177/0278364917735594},
  Eprint                   = {https://doi.org/10.1177/0278364917735594},
  Number                   = {13-14},
  Pages                    = {1455-1473},
  Url                      = {https://doi.org/10.1177/0278364917735594},
  Volume                   = {36},

  Abstract                 = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter. },
  Journal                  = {The International Journal of Robotics Research}
}

@Article{Porzi2017,
  Title                    = {Learning Depth-Aware Deep Representations for Robotic Perception},
  Author                   = {L. Porzi and S. R. Buló and A. Penate-Sanchez and E. Ricci and F. Moreno-Noguer},
  Date                     = {2017-04},
  Journaltitle             = {IEEE Robotics and Automation Letters},
  Year                     = {2017},
  Doi                      = {10.1109/LRA.2016.2637444},
  Month                    = {April},
  Number                   = {2},
  Pages                    = {468-475},
  Volume                   = {2},

  Abstract                 = {Exploiting RGB-D data by means of convolutional neural networks (CNNs) is at the core of a number of robotics applications, including object detection, scene semantic segmentation, and grasping. Most existing approaches, however, exploit RGB-D data by simply considering depth as an additional input channel for the network. In this paper we show that the performance of deep architectures can be boosted by introducing DaConv, a novel, general-purpose CNN block which exploits depth to learn scale-aware feature representations. We demonstrate the benefits of DaConv on a variety of robotics oriented tasks, involving affordance detection, object coordinate regression, and contour detection in RGB-D images. In each of these experiments we show the potential of the proposed block and how it can be readily integrated into existing CNN architectures.},
  Journal                  = {IEEE Robotics and Automation Letters},
  Keywords                 = {convolution;image colour analysis;image representation;image segmentation;neural net architecture;object detection;regression analysis;robot vision;CNN architectures;DaConv;RGB-D data;RGB-D images;affordance detection;contour detection;convolutional neural networks;deep architectures;depth-aware deep representations;general-purpose CNN block;grasping;object coordinate regression;object detection;robotic perception;scale-aware feature representations;scene semantic segmentation;Computer architecture;Convolution;Kernel;Robot kinematics;Standards;Three-dimensional displays;RGB-D perception;visual learning}
}

@Inproceedings{Qi2016,
  Title                    = {Volumetric and Multi-view CNNs for Object Classification on 3D Data},
  Author                   = {C. R. Qi and H. Su and M. Nießner and A. Dai and M. Yan and L. J. Guibas},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Date                     = {2016-06},
  Editor                   = {Lourdes Agapito and Tamara Berg and Jana Kosecka and Lihi Zelnik-Manor},
  Year                     = {2016},
  Doi                      = {10.1109/CVPR.2016.609},
  Month                    = {June},
  Pages                    = {5648-5656},

  Abstract                 = {3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-theart methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multiresolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.},
  Keywords                 = {image classification;neural nets;stereo image processing;3D data;3D multiresolution filtering;3D representations;3D shape models;multi-view representations;multiview CNN;network architectures;object classification;volumetric CNN;volumetric representations;Computer architecture;Rendering (computer graphics);Shape;Solid modeling;Three-dimensional displays;Training;Two dimensional displays}
}

@Article{Roa2015,
  Title                    = {Grasp Quality Measures: Review and Performance},
  Author                   = {Roa, M\'{a}ximo A. and Su\'{a}rez, Ra\'{u}l},
  Date                     = {2015-01},
  Journaltitle             = {Autonomous Robots},
  Year                     = {2015},
  Doi                      = {10.1007/s10514-014-9402-3},
  ISSN                     = {0929-5593},
  Month                    = jan,
  Number                   = {1},
  Pages                    = {65--88},
  Volume                   = {38},

  Abstract                 = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
  Acmid                    = {2720569},
  Address                  = {Hingham, MA, USA},
  Issue_date               = {January 2015},
  Journal                  = {Autonomous Robots},
  Keywords                 = {Grasp quality, Grasping, Manipulation, Robotic hands},
  Numpages                 = {24},
  Publisher                = {Kluwer Academic Publishers}
}

@Inproceedings{Rubert2017,
  Title                    = {On the relevance of grasp metrics for predicting grasp success},
  Author                   = {C. Rubert and D. Kappler and A. Morales and S. Schaal and J. Bohg},
  Booktitle                = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Date                     = {2017-09},
  Editor                   = {Tony Maciejewski},
  Year                     = {2017},
  Doi                      = {10.1109/IROS.2017.8202167},
  Month                    = {Sept},
  Pages                    = {265-272},

  Abstract                 = {We aim to reliably predict whether a grasp on a known object is successful before it is executed in the real world. There is an entire suite of grasp metrics that has already been developed which rely on precisely known contact points between object and hand. However, it remains unclear whether and how they may be combined into a general purpose grasp stability predictor. In this paper, we analyze these questions by leveraging a large scale database of simulated grasps on a wide variety of objects. For each grasp, we compute the value of seven metrics. Each grasp is annotated by human subjects with ground truth stability labels. Given this data set, we train several classification methods to find out whether there is some underlying, non-trivial structure in the data that is difficult to model manually but can be learned. Quantitative and qualitative results show the complexity of the prediction problem. We found that a good prediction performance critically depends on using a combination of metrics as input features. Furthermore, non-parametric and non-linear classifiers best capture the structure in the data.},
  Keywords                 = {grippers;pattern classification;classification methods;contact points;general purpose grasp stability predictor;grasp metrics;grasp success prediction;ground truth stability labels;large scale database;nonlinear classifiers;nonparametric classifiers;prediction performance;prediction problem complexity;Computational modeling;Databases;Labeling;Measurement;Physics;Robots;Stability analysis}
}

@Article{Sahbani2012,
  Title                    = {An Overview of 3D Object Grasp Synthesis Algorithms},
  Author                   = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
  Date                     = {2012-03-01},
  Journaltitle             = {Robotics and Autonomous Systems},
  Year                     = {2012},
  Doi                      = {10.1016/j.robot.2011.07.016},
  ISSN                     = {0921-8890},
  Month                    = mar,
  Number                   = {3},
  Pages                    = {326--336},
  Volume                   = {60},

  Abstract                 = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger-object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
  Acmid                    = {2109859},
  Address                  = {Amsterdam, The Netherlands, The Netherlands},
  Issue_date               = {March, 2012},
  Journal                  = {Robotics and Autonomous Systems},
  Keywords                 = {Force-closure, Grasp synthesis, Learning by demonstration, Task modeling},
  Numpages                 = {11},
  Publisher                = {North-Holland Publishing Co.}
}

@Article{Saudabayev2018,
  Title                    = {Human grasping database for activities of daily living with depth, color and kinematic data streams},
  Author                   = {Saudabayev, Artur
and Rysbek, Zhanibek
and Khassenova, Raykhan
and Varol, Huseyin Atakan},
  Date                     = {2018-05-29},
  Journaltitle             = {Scientific Data},
  Year                     = {2018},
  Month                    = {May},
  Note                     = {Data Descriptor},
  Pages                    = {180101},
  Volume                   = {5},

  Abstract                 = {This paper presents a grasping database collected from multiple human subjects for activities of daily living in unstructured environments. The main strength of this database is the use of three different sensing modalities: color images from a head-mounted action camera, distance data from a depth sensor on the dominant arm and upper body kinematic data acquired from an inertial motion capture suit. 3826 grasps were identified in the data collected during 9-hours of experiments. The grasps were grouped according to a hierarchical taxonomy into 35 different grasp types. The database contains information related to each grasp and associated sensor data acquired from the three sensor modalities. We also provide our data annotation software written in Matlab as an open-source tool. The size of the database is 172 GB. We believe this database can be used as a stepping stone to develop big data and machine learning techniques for grasping and manipulation with potential applications in rehabilitation robotics and intelligent automation.},
  Day                      = {29},
  Journal                  = {Scientific Data}
}

@Article{Shimoga1996,
  Title                    = {Robot Grasp Synthesis Algorithms: A Survey},
  Author                   = {K.B. Shimoga},
  Date                     = {1996-06-01},
  Journaltitle             = {The International Journal of Robotics Research},
  Year                     = {1996},
  Doi                      = {10.1177/027836499601500302},
  Eprint                   = { 
 https://doi.org/10.1177/027836499601500302
 
},
  Number                   = {3},
  Pages                    = {230-266},
  Volume                   = {15},

  Abstract                 = { This article presents a survey of the existing computational algorithms meant for achieving four important properties in autonomous multifingered robotic hands. The four properties are: dexterity, equilibrium, stability, and dynamic behavior The multifingered robotic hands must be controlled so as to possess these properties and hence be able to autonomously perform complex tasks in a way similar to human hands.Existing algorithms to achieve dexterity primarily involve solving an unconstrained linear programming problem where an objective function can be chosen to represent one or more of the currently known dexterity measures. Algorithms to achieve equilibrium also constitute solving a linear program ming problem wherein the positivity, friction, and joint torque constraints of all fingers are accounted for while optimizing the internal grasping forces. Stability algorithms aim at achiev ing positive definite grasp impedance matrices by solving for the required fingertip impedances. This problem reduces to a nonlinear programming problem. Dynamic behavior algorithms determine fingertip impedances, which, when achieved, lead to a desired dynamic behavior. This problem, too, becomes a linear programming problem.If a robotic hand has to acquire any or all of these proper ties, the corresponding algorithms should become integral parts of the hand control system. These algorithms are collectively referred to in this article as robot grasp synthesis algorithms. },
  Journal                  = {The International Journal of Robotics Research}
}

@Inproceedings{Shrivastava2017,
  Title                    = {Learning from Simulated and Unsupervised Images through Adversarial Training},
  Author                   = {A. Shrivastava and T. Pfister and O. Tuzel and J. Susskind and W. Wang and R. Webb},
  Booktitle                = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Date                     = {2017-07},
  Editor                   = {Lisa O’Conner},
  Year                     = {2017},
  Doi                      = {10.1109/CVPR.2017.241},
  Month                    = {July},
  Pages                    = {2242-2251},

  Abstract                 = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  ISSN                     = {1063-6919},
  Keywords                 = {pose estimation;realistic images;unsupervised learning;Generative Adversarial Networks;MPIIGaze dataset;Simulated+Unsupervised learning;gaze estimation;hand pose estimation;synthetic image distributions;Computational modeling;Data models;Gallium nitride;Neural networks;Pose estimation;Training}
}

@Inproceedings{Su2015,
  Title                    = {Multi-view Convolutional Neural Networks for 3D Shape Recognition},
  Author                   = {H. Su and S. Maji and E. Kalogerakis and E. Learned-Miller},
  Booktitle                = {2015 IEEE International Conference on Computer Vision (ICCV)},
  Date                     = {2015-12},
  Editor                   = {Katsushi Ikeuchi and Christoph Schnörr and Josef Sivic and Rene Vidal},
  Year                     = {2015},
  Doi                      = {10.1109/ICCV.2015.114},
  Month                    = {Dec},
  Pages                    = {945-953},

  Abstract                 = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
  Keywords                 = {computer vision;convolution;image representation;learning (artificial intelligence);mesh generation;neural nets;rendering (computer graphics);shape recognition;2D images;3D shape descriptors;3D shape recognition;3D shape representation;computer vision;hand-drawn shape sketch recognition;learning;multiview convolutional neural networks;polygon mesh;shape rendered views;view-based descriptors;voxel grid;Cameras;Computer architecture;Computer vision;Image recognition;Shape;Solid modeling;Three-dimensional displays}
}

@Book{suarez2006,
  Title                    = {Grasp quality measures},
  Author                   = {Su{\'a}rez, Ra{\'u}l and Cornella, Jordi and Garz{\'o}n, M{\'a}ximo Roa},
  Date                     = {2006},
  Year                     = {2006},
  Publisher                = {Institut d'Organitzaci{\'o} i Control de Sistemes Industrials}
}

@Inproceedings{Varley2017,
  Title                    = {Shape completion enabled robotic grasping},
  Author                   = {J. Varley and C. DeChant and A. Richardson and J. Ruales and P. Allen},
  Booktitle                = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Date                     = {2017-09},
  Editor                   = {Tony Maciejewski},
  Year                     = {2017},
  Doi                      = {10.1109/IROS.2017.8206060},
  Month                    = {Sept},
  Pages                    = {2442-2447},

  Abstract                 = {This work provides an architecture to enable robotic grasp planning via shape completion. Shape completion is accomplished through the use of a 3D convolutional neural network (CNN). The network is trained on our own new open source dataset of over 440,000 3D exemplars captured from varying viewpoints. At runtime, a 2.5D pointcloud captured from a single point of view is fed into the CNN, which fills in the occluded regions of the scene, allowing grasps to be planned and executed on the completed object. Runtime shape completion is very rapid because most of the computational costs of shape completion are borne during offline training. We explore how the quality of completions vary based on several factors. These include whether or not the object being completed existed in the training data and how many object models were used to train the network. We also look at the ability of the network to generalize to novel objects allowing the system to complete previously unseen objects at runtime. Finally, experimentation is done both in simulation and on actual robotic hardware to explore the relationship between completion quality and the utility of the completed mesh model for grasping.},
  Keywords                 = {learning (artificial intelligence);mesh generation;neural nets;object recognition;robot programming;solid modelling;3D CNN training;3D convolutional neural network;completed object;completion quality;mesh model;robotic grasp planning;runtime shape completion;Databases;Planning;Robots;Runtime;Shape;Three-dimensional displays;Training}
}

@Inproceedings{WeiszAllen2012,
  Title                    = {Pose error robust grasping from contact wrench space metrics},
  Author                   = {J. Weisz and P. K. Allen},
  Booktitle                = {2012 IEEE International Conference on Robotics and Automation},
  Date                     = {2012-05},
  Editor                   = {Antonio Bicchi},
  Year                     = {2012},
  Doi                      = {10.1109/ICRA.2012.6224697},
  Month                    = {May},
  Pages                    = {557-562},

  Abstract                 = {Grasp quality metrics which analyze the contact wrench space are commonly used to synthesize and analyze preplanned grasps. Preplanned grasping approaches rely on the robustness of stored solutions. Analyzing the robustness of such solutions for large databases of preplanned grasps is a limiting factor for the applicability of data driven approaches to grasping. In this work, we will focus on the stability of the widely used grasp wrench space epsilon quality metric over a large range of poses in simulation. We examine a large number of grasps from the Columbia Grasp Database for the Barrett hand. We find that in most cases the grasp with the most robust force closure with respect to pose error for a particular object is not the grasp with the highest epsilon quality. We demonstrate that grasps can be reranked by an estimate of the stability of their epsilon quality. We find that the grasps ranked best by this method are successful more often in physical experiments than grasps ranked best by the epsilon quality.},
  ISSN                     = {1050-4729},
  Keywords                 = {grippers;robust control;Barrett hand;Columbia grasp database;contact wrench space metrics;data driven approach;grasp wrench space epsilon quality metric;pose error robust grasping;robust force closure;robustness analysis;Databases;Force;Grasping;Joints;Measurement;Robustness;Uncertainty}
}

@Inproceedings{Yang2017,
  Title                    = {3D Object Reconstruction from a Single Depth View with Adversarial Learning},
  Author                   = {Yang, Bo and Wen, Hongkai and Wang, Sen and Clark, Ronald and Markham, Andrew and Trigoni, Niki},
  Booktitle                = {The IEEE International Conference on Computer Vision (ICCV) Workshops},
  Date                     = {2017-10},
  Editor                   = {Cucchiara, R. and Matsushita, Y. and Sebe, N. and Soatto, S.},
  Year                     = {2017},
  Month                    = {Oct}
}

@Book{Yoshikawa1990,
  Title                    = {Foundations of Robotics: Analysis and Control},
  Author                   = {Yoshikawa, Tsuneo},
  Date                     = {1990},
  Year                     = {1990},
  ISBN                     = {0-262-24028-9},
  Publisher                = {MIT Press},

  Address                  = {Cambridge, MA, USA}
}

@Inproceedings{YuKoltun2016,
  Title                    = {Multi-Scale Context Aggregation by Dilated Convolutions},
  Author                   = {Fisher Yu and Vladlen Koltun},
  Booktitle                = {ICLR},
  Date                     = {2016-05},
  Editor                   = {Hugo Larochelle and Oriol Vinyals and Tara Sainath},
  Year                     = {2016}
}

